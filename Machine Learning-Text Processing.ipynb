{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Popular Tasks Regarding Text Processing:\n",
    "\n",
    "* **Language Translation** - Translation of a sentence from one language to another\n",
    "* **Sentiment Analysis** - To determine whether the sentiment towards any topic or product is positive, negative or neutral, based on a corpus of text\n",
    "* **Spam Filtering** - To detect unsolicited and unwanted email/messages\n",
    "\n",
    "In this notebook, we'll discuss the steps involved in text processing.\n",
    "\n",
    "# Data Preprocessing\n",
    "***\n",
    "The data preprocessing steps could include:\n",
    "* **Tokenization** - converting sentences to words\n",
    "* Removing unnecessary punctuation and tags\n",
    "* Removing stop words\n",
    "* Stemming - Removing inflection via dropping unnecessary characters (usually a suffix)\n",
    "* Lemmatization - Removing inflection by determining the part of speech and utilized a detailed database of the language\n",
    "\n",
    "Stemming is the poor man's lemmatization\n",
    "\n",
    "```text\n",
    "The stemmed form of studies is: studi\n",
    "The stemmed form of studying is: study\n",
    "\n",
    "The lemmatized form of studies is: study\n",
    "The lemmatized form of studying is: study\n",
    "```\n",
    "\n",
    "We can use the `nltk` library to do a lot of text preprocessing:\n",
    "## Tokenization\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize( text )\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "***\n",
    "We can use `nltk` to remove stop words (words containing no semantic value)\n",
    "\n",
    "If you need to download the stopwords for `nltk`, you need to run:\n",
    "```python\n",
    "nltk.download(\"stopwords\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "***\n",
    "`nltk` also provides several stemmer interfaces like Porter stemmer, Lancaster stemmer and snowball stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stems = []\n",
    "for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "***\n",
    "In text processing, words represent discrete, categorical features. How do we encode this categorical data in a way that is ready to be used by the algorithms? One of the simples techniques is the **bag of words** featurization.\n",
    "\n",
    "## Bag of Words (BOW)\n",
    "***\n",
    "We make a **list of unique words** in the corpus, called the **vocabulary**. Then each word in the vocabulary gets its own basis vector. Then a sentence, or each document in the corpus is represented by a vector that is equal to the vector sums of the basis vectors for the words appearing in that sentence. \n",
    "\n",
    "This leads us to the **Term Frequency-Inverse Document Frequency (TF-IDF)** technique:\n",
    "\n",
    "## TF-IDF\n",
    "***\n",
    "First, let us clarify what is meant by document. A corpus is made up of many documents. So if your corpus is a set of tweets, each document is a tweet. So we can ask for the word vector of an entire tweet. The word vector of an entire tweet would be some function of the word vectors of its constituent words.\n",
    "\n",
    "$$\\textrm{Term Frequency (TF)} = \\frac{\\textrm{number of times token appears in single document}}{\\textrm{number of tokens in single document}}$$\n",
    "\n",
    "$$\\textrm{Inverse Document Frequency (IDF)} = \\log\\left(\\frac{\\textrm{total number documents}}{\\textrm{number of documents this token appears in}}\\right)$$\n",
    "\n",
    "$$\\textrm{TF-IDF} = (\\textrm{TF})(\\textrm{IDF})$$\n",
    "\n",
    "Here is an example of calculating the TF-IDF of a term in a document:\n",
    "\n",
    "```text\n",
    "tweet_one = \"This is a beautiful beautiful day day day day day\"\n",
    "tweet_two = \"This is a beautiful night night\"\n",
    "```\n",
    "\n",
    "Then we would have that:\n",
    "```text\n",
    "TF(\"beautiful\",tweet_one) = 2/10\n",
    "TF(\"day\", tweet_one) = 5/10\n",
    "IDF(\"beautiful\") = log(2/2) = 0\n",
    "IDF(\"day\") = log(2/1) = 0.3\n",
    "\n",
    "TF_IDF(\"beautiful\", tweet_one) = (2/10)(0)=0\n",
    "TF_IDF(\"day\", tweet_one) = (5/10)(0.30) = 0.15\n",
    "```\n",
    "\n",
    "So we see that for the first tweet, the TF-IDF method heavily penalizes the word \"beautiful\", but assigns greater weight to \"day\". \"beautiful\" gives no power of resolution because it appears in both documents. \"day\" is an important word for `tweet_one` in the context of the entire corpus (it only appears in one of the two tweets). \n",
    "\n",
    "`scikit-learn` provides efficient tools for computing the TF-IDF of a corpus.\n",
    "\n",
    "One of the major disadvantages of using the bag of words featurization is that it discards all information contained in the word order of the vectors. \n",
    "\n",
    "To solve this problem, we use an approach called **Word Embedding**\n",
    "## Word Embedding\n",
    "***\n",
    "A word embedding is a numerical representation of text where words that have similar semantic meaning are geometrically closer in their semantic vector space. \n",
    "\n",
    "### Word2Vec\n",
    "***\n",
    "Word2Vec takes a corpus of text and produces a vector space that has the property that vectors that are geometrically close also share common semantic contexts in the corpus. \n",
    "\n",
    "### Glove\n",
    "***\n",
    "The **Global Vectors for Word Representation** algorithm is an extension to the Word2Vec model. GloVe constructs a co-occurrence matrix on the whole text corpus. The entries of this matrix are the probabilities that a given token appears in the common of every other word in the vocabulary. This assumes the **distributional hypothesis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing ML Algorithms\n",
    "***\n",
    "Classical ML approaches such as Naive bayes or Support Vector machines for spam filtering are very popular. However, Deep Learning techniques, combined with deep-learned word embeddings are taking the NLP stage by storm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example IMDB Movie Review Sentiment Analyzer\n",
    "***\n",
    "We will now build a sentiment analyzer over the IMDB movie review dataset. \n",
    "\n",
    "We will be performing binary classification (negative or positive reviews).\n",
    "\n",
    "Our dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "The dataset contains 25000 training reviews and 25000 testing reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/imdb_movie_reviews/test/pos/9013_10.txt\n",
      "data/imdb_movie_reviews/test/pos/9849_8.txt\n",
      "data/imdb_movie_reviews/test/pos/10012_9.txt\n",
      "data/imdb_movie_reviews/test/pos/941_10.txt\n",
      "data/imdb_movie_reviews/test/pos/3012_7.txt\n",
      "data/imdb_movie_reviews/test/pos/119_9.txt\n",
      "data/imdb_movie_reviews/test/pos/4619_10.txt\n",
      "data/imdb_movie_reviews/test/pos/40_8.txt\n",
      "data/imdb_movie_reviews/test/pos/8975_8.txt\n",
      "data/imdb_movie_reviews/test/pos/3046_8.txt\n",
      "data/imdb_movie_reviews/test/pos/5143_8.txt\n",
      "data/imdb_movie_reviews/test/pos/8296_8.txt\n",
      "data/imdb_movie_reviews/test/pos/268_10.txt\n",
      "data/imdb_movie_reviews/test/pos/5419_10.txt\n",
      "data/imdb_movie_reviews/test/pos/10986_10.txt\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-54b4b9352b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreliminary_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             review = pd.read_csv( os.path.join(preliminary_path, file) ,header=None, \\\n\u001b[0;32m---> 15\u001b[0;31m                                  names=[\"review\"], encoding='utf8')\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"data/imdb_movie_reviews/\"\n",
    "labels = {'pos':1, 'neg':0}\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for dataset in [\"test\",\"train\"]:\n",
    "    for polarity in [\"pos\",\"neg\"]:\n",
    "        preliminary_path = os.path.join(folder, dataset, polarity)\n",
    "        for file in os.listdir(preliminary_path):\n",
    "            print(os.path.join(preliminary_path, file))\n",
    "            review = pd.read_csv( os.path.join(preliminary_path, file) ,header=None, \\\n",
    "                                 names=[\"review\"], encoding='utf8')\n",
    "            review[\"sentiment\"] = labels[polarity]\n",
    "            data.append(review)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
