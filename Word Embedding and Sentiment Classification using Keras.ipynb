{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Popular Tasks Regarding Text Processing:\n",
    "\n",
    "* **Language Translation** - Translation of a sentence from one language to another\n",
    "* **Sentiment Analysis** - To determine whether the sentiment towards any topic or product is positive, negative or neutral, based on a corpus of text\n",
    "* **Spam Filtering** - To detect unsolicited and unwanted email/messages\n",
    "\n",
    "In this notebook, we'll discuss the steps involved in text processing.\n",
    "\n",
    "# Data Preprocessing\n",
    "***\n",
    "The data preprocessing steps could include:\n",
    "* **Tokenization** - converting sentences to words\n",
    "* Removing unnecessary punctuation and tags\n",
    "* Removing stop words\n",
    "* Stemming - Removing inflection via dropping unnecessary characters (usually a suffix)\n",
    "* Lemmatization - Removing inflection by determining the part of speech and utilized a detailed database of the language\n",
    "\n",
    "Stemming is the poor man's lemmatization\n",
    "\n",
    "```text\n",
    "The stemmed form of studies is: studi\n",
    "The stemmed form of studying is: study\n",
    "\n",
    "The lemmatized form of studies is: study\n",
    "The lemmatized form of studying is: study\n",
    "```\n",
    "\n",
    "We can use the `nltk` library to do a lot of text preprocessing:\n",
    "## Tokenization\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize( text )\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords\n",
    "***\n",
    "We can use `nltk` to remove stop words (words containing no semantic value)\n",
    "\n",
    "If you need to download the stopwords for `nltk`, you need to run:\n",
    "```python\n",
    "nltk.download(\"stopwords\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "***\n",
    "`nltk` also provides several stemmer interfaces like Porter stemmer, Lancaster stemmer and snowball stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stems = []\n",
    "for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "***\n",
    "In text processing, words represent discrete, categorical features. How do we encode this categorical data in a way that is ready to be used by the algorithms? One of the simples techniques is the **bag of words** featurization.\n",
    "\n",
    "## Bag of Words (BOW)\n",
    "***\n",
    "We make a **list of unique words** in the corpus, called the **vocabulary**. Then each word in the vocabulary gets its own basis vector. Then a sentence, or each document in the corpus is represented by a vector that is equal to the vector sums of the basis vectors for the words appearing in that sentence. \n",
    "\n",
    "This leads us to the **Term Frequency-Inverse Document Frequency (TF-IDF)** technique:\n",
    "\n",
    "## TF-IDF\n",
    "***\n",
    "First, let us clarify what is meant by document. A corpus is made up of many documents. So if your corpus is a set of tweets, each document is a tweet. So we can ask for the word vector of an entire tweet. The word vector of an entire tweet would be some function of the word vectors of its constituent words.\n",
    "\n",
    "$$\\textrm{Term Frequency (TF)} = \\frac{\\textrm{number of times token appears in single document}}{\\textrm{number of tokens in single document}}$$\n",
    "\n",
    "$$\\textrm{Inverse Document Frequency (IDF)} = \\log\\left(\\frac{\\textrm{total number documents}}{\\textrm{number of documents this token appears in}}\\right)$$\n",
    "\n",
    "$$\\textrm{TF-IDF} = (\\textrm{TF})(\\textrm{IDF})$$\n",
    "\n",
    "Here is an example of calculating the TF-IDF of a term in a document:\n",
    "\n",
    "```text\n",
    "tweet_one = \"This is a beautiful beautiful day day day day day\"\n",
    "tweet_two = \"This is a beautiful night night\"\n",
    "```\n",
    "\n",
    "Then we would have that:\n",
    "```text\n",
    "TF(\"beautiful\",tweet_one) = 2/10\n",
    "TF(\"day\", tweet_one) = 5/10\n",
    "IDF(\"beautiful\") = log(2/2) = 0\n",
    "IDF(\"day\") = log(2/1) = 0.3\n",
    "\n",
    "TF_IDF(\"beautiful\", tweet_one) = (2/10)(0)=0\n",
    "TF_IDF(\"day\", tweet_one) = (5/10)(0.30) = 0.15\n",
    "```\n",
    "\n",
    "So we see that for the first tweet, the TF-IDF method heavily penalizes the word \"beautiful\", but assigns greater weight to \"day\". \"beautiful\" gives no power of resolution because it appears in both documents. \"day\" is an important word for `tweet_one` in the context of the entire corpus (it only appears in one of the two tweets). \n",
    "\n",
    "`scikit-learn` provides efficient tools for computing the TF-IDF of a corpus.\n",
    "\n",
    "One of the major disadvantages of using the bag of words featurization is that it discards all information contained in the word order of the vectors. \n",
    "\n",
    "To solve this problem, we use an approach called **Word Embedding**\n",
    "## Word Embedding\n",
    "***\n",
    "A word embedding is a numerical representation of text where words that have similar semantic meaning are geometrically closer in their semantic vector space. \n",
    "\n",
    "### Word2Vec\n",
    "***\n",
    "Word2Vec takes a corpus of text and produces a vector space that has the property that vectors that are geometrically close also share common semantic contexts in the corpus. \n",
    "\n",
    "### Glove\n",
    "***\n",
    "The **Global Vectors for Word Representation** algorithm is an extension to the Word2Vec model. GloVe constructs a co-occurrence matrix on the whole text corpus. The entries of this matrix are the probabilities that a given token appears in the common of every other word in the vocabulary. This assumes the **distributional hypothesis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing ML Algorithms\n",
    "***\n",
    "Classical ML approaches such as Naive bayes or Support Vector machines for spam filtering are very popular. However, Deep Learning techniques, combined with deep-learned word embeddings are taking the NLP stage by storm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example IMDB Movie Review Sentiment Analyzer\n",
    "***\n",
    "We will now build a sentiment analyzer over the IMDB movie review dataset. \n",
    "\n",
    "We will be performing binary classification (negative or positive reviews).\n",
    "\n",
    "Our dataset can be downloaded [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "The dataset contains 25000 training reviews and 25000 testing reviews. \n",
    "\n",
    "## Loading the Data\n",
    "***\n",
    "The data is separated into test/pos, test/neg, train/pos, train/neg folders containing text files for each review. We need to load all of these reviews into a dataframe so we can do stuff with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-12 17:14:54,166 : INFO : read 10000 reviews\n",
      "2019-08-12 17:15:08,754 : INFO : read 20000 reviews\n",
      "2019-08-12 17:15:25,529 : INFO : read 30000 reviews\n",
      "2019-08-12 17:15:45,038 : INFO : read 40000 reviews\n",
      "2019-08-12 17:16:08,094 : INFO : read 50000 reviews\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\",\\\n",
    "                   level=logging.INFO)\n",
    "\n",
    "folder = \"data/imdb_movie_reviews/\"\n",
    "labels = {'pos':1, 'neg':0}\n",
    "\n",
    "data = pd.DataFrame()\n",
    "i = 0\n",
    "for dataset in [\"test\",\"train\"]:\n",
    "    for polarity in [\"pos\",\"neg\"]:\n",
    "        preliminary_path = os.path.join(folder, dataset, polarity)\n",
    "        for file in os.listdir(preliminary_path):\n",
    "            full_file_name = os.path.join(preliminary_path, file)\n",
    "            with open(full_file_name, 'r', encoding='utf-8') as fileHandle:\n",
    "                review_text = fileHandle.read()\n",
    "                review = pd.DataFrame([{\"review\":review_text,\"sentiment\":labels[polarity]}])\n",
    "            i += 1\n",
    "            data = data.append(review, ignore_index=True)\n",
    "            if (i%10000==0):\n",
    "                logging.info(\"read {} reviews\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original Movie lovers can actually love this s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peak Practice was a British drama series about...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My wife is a mental health therapist and we wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A fantastic film featuring great Aussie talent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Despite all it's trappings of style and cinema...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I finally watched the third film in Mehta's tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I saw this movie tonight in a preview showing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lackawana Blues An impressive HBO movie about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aaron Sorking raises the same questions as Sha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Martin Ritt seems to be a director who was alw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>In 'Hoot' Logan Lerman plays Roy Eberhardt, th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The family happiness is crumbling when, the fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I'm surprised that mine, so far, is the only c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Just went on YouTube and finally watched this ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Two things can happen when an ensemble cast is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was very impressed with this film from newco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Capt. Corelli's Mandolin\" is an old fashioned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Enigma is a computer part which scrambles Russ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This is such an exciting documentary, it was b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Robert Florey and James Wong Howe gave this a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Writer/director John Milius takes a little-kno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This is a great Italian shark movie probably n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Everything about this movie is perfect. The se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>For a long time, this was my favorite of the B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The directing is brilliant, the casting is rem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>In all truth, this really isn't a \"movie\" so m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I have been looking for this mini-series for a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A mixture of Alien and Ghost Busters?&lt;br /&gt;&lt;br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>This is one of those movies that are very unde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>While I agree completely with drvn below about...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>Well, first off, if you're checking out Revolt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>I read comments about this being the best Chin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>Contrary to most other commentators, I deeply ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>When I saw that this movie was being shown on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>Some of my old friends suggested me to watch t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>A movie about dealing with the problems with g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>Well our standards have gone into the toilet. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>I'm shocked that there were people who liked t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>I'm not going to criticize the movie. There is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>This is a bad, bad movie. I'm an actual fencer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>I'm afraid that I didn't like this movie very ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>I saw this not too long ago, and I must say: T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>I enjoy quality crapness, and this ranks up th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I didn't see They Call Me Trinity,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>Well, what can it be said about this disaster?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>In a word, this film was boring. It lacked lif...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>I don't know much about film-making, but good ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>OK, so obviously ppl thought this was a good m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>This is one of the silliest movies I have ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>How this movie got made with a supposedly $70 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>I have read several good reviews that have def...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>This movie had all the elements to be a smart,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>Now, I flicked onto this just out of curiosity...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>I tuned into this thing one night on a cable c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Being an Elvis fan, I can't understand how thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>I'm a Christian. I have always been skeptical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I remember when this piece of trash came out, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>For the first time in years, I've felt the nee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Just picked up this film for a buck at Nationa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      Original Movie lovers can actually love this s...          1\n",
       "1      Peak Practice was a British drama series about...          1\n",
       "2      My wife is a mental health therapist and we wa...          1\n",
       "3      A fantastic film featuring great Aussie talent...          1\n",
       "4      Despite all it's trappings of style and cinema...          1\n",
       "5      I finally watched the third film in Mehta's tr...          1\n",
       "6      I saw this movie tonight in a preview showing ...          1\n",
       "7      Lackawana Blues An impressive HBO movie about ...          1\n",
       "8      Aaron Sorking raises the same questions as Sha...          1\n",
       "9      Martin Ritt seems to be a director who was alw...          1\n",
       "10     In 'Hoot' Logan Lerman plays Roy Eberhardt, th...          1\n",
       "11     The family happiness is crumbling when, the fr...          1\n",
       "12     I'm surprised that mine, so far, is the only c...          1\n",
       "13     Just went on YouTube and finally watched this ...          1\n",
       "14     Two things can happen when an ensemble cast is...          1\n",
       "15     I was very impressed with this film from newco...          1\n",
       "16     \"Capt. Corelli's Mandolin\" is an old fashioned...          1\n",
       "17     Enigma is a computer part which scrambles Russ...          1\n",
       "18     This is such an exciting documentary, it was b...          1\n",
       "19     Robert Florey and James Wong Howe gave this a ...          1\n",
       "20     Writer/director John Milius takes a little-kno...          1\n",
       "21     This is a great Italian shark movie probably n...          1\n",
       "22     Everything about this movie is perfect. The se...          1\n",
       "23     For a long time, this was my favorite of the B...          1\n",
       "24     The directing is brilliant, the casting is rem...          1\n",
       "25     In all truth, this really isn't a \"movie\" so m...          1\n",
       "26     I have been looking for this mini-series for a...          1\n",
       "27     A mixture of Alien and Ghost Busters?<br /><br...          1\n",
       "28     This is one of those movies that are very unde...          1\n",
       "29     While I agree completely with drvn below about...          1\n",
       "...                                                  ...        ...\n",
       "49970  Well, first off, if you're checking out Revolt...          0\n",
       "49971  I read comments about this being the best Chin...          0\n",
       "49972  Contrary to most other commentators, I deeply ...          0\n",
       "49973  When I saw that this movie was being shown on ...          0\n",
       "49974  Some of my old friends suggested me to watch t...          0\n",
       "49975  A movie about dealing with the problems with g...          0\n",
       "49976  Well our standards have gone into the toilet. ...          0\n",
       "49977  I'm shocked that there were people who liked t...          0\n",
       "49978  I'm not going to criticize the movie. There is...          0\n",
       "49979  This is a bad, bad movie. I'm an actual fencer...          0\n",
       "49980  I've seen some crappy movies in my life, but t...          0\n",
       "49981  I'm afraid that I didn't like this movie very ...          0\n",
       "49982  I saw this not too long ago, and I must say: T...          0\n",
       "49983  I enjoy quality crapness, and this ranks up th...          0\n",
       "49984  <br /><br />I didn't see They Call Me Trinity,...          0\n",
       "49985  Well, what can it be said about this disaster?...          0\n",
       "49986  In a word, this film was boring. It lacked lif...          0\n",
       "49987  I don't know much about film-making, but good ...          0\n",
       "49988  OK, so obviously ppl thought this was a good m...          0\n",
       "49989  This is one of the silliest movies I have ever...          0\n",
       "49990  How this movie got made with a supposedly $70 ...          0\n",
       "49991  I have read several good reviews that have def...          0\n",
       "49992  This movie had all the elements to be a smart,...          0\n",
       "49993  Now, I flicked onto this just out of curiosity...          0\n",
       "49994  I tuned into this thing one night on a cable c...          0\n",
       "49995  Being an Elvis fan, I can't understand how thi...          0\n",
       "49996  I'm a Christian. I have always been skeptical ...          0\n",
       "49997  I remember when this piece of trash came out, ...          0\n",
       "49998  For the first time in years, I've felt the nee...          0\n",
       "49999  Just picked up this film for a buck at Nationa...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Frequency of Top Words\n",
    "***\n",
    "We can get the frequency distribution of the words in the text by utilizing the `nltk.FreqDist()` function. This function lists the top words used in the text. We can look at the top 50 most frequent words in the text: As you can see, we have a lot of stop words in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in vocabulary: 198758\n",
      "['the', ',', '.', 'a', 'and', 'of', 'to', 'is', '/', '>', '<', 'br', 'in', 'I', 'it', 'that', \"'s\", 'this', 'was', 'The', 'as', 'with', 'movie', 'for', 'film', ')', '(', 'but', \"''\", \"n't\", '``', 'on', 'you', 'are', 'not', 'have', 'his', 'be', '!', 'he', 'one', 'at', 'by', 'an', 'all', 'who', 'they', 'from', 'like', 'It']\n"
     ]
    }
   ],
   "source": [
    "# all of the words in the corpus concatenated\n",
    "reviews = data.review.str.cat(sep=\" \")\n",
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(\"Number of unique words in vocabulary: {}\".format(len(vocabulary)))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "print(sorted(frequency_dist, key=frequency_dist.__getitem__, reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords\n",
    "***\n",
    "Now we shall remove stopwords to further clean up the text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', '/', '>', '<', 'br', 'I', \"'s\", 'The', 'movie', 'film', ')', '(', \"''\", \"n't\", '``', '!', 'one', 'like', 'It', '?', 'This', 'good', 'would', '...', 'time', 'really', 'see', 'even', 'story', \"'\", ':', '-', 'much', 'could', 'get', 'people', 'bad', 'great', 'well', 'first', 'made', 'also', 'make', 'way', 'movies', 'But', 'think', 'characters', 'character']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "print(sorted(frequency_dist, key=frequency_dist.__getitem__, reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud Visualization\n",
    "***\n",
    "Now we can make a wordcloud visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Classifier\n",
    "***\n",
    "Now we can divide the dataset into a test and training set of 25000 reviews each, appropriately stratified by label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.review,data.sentiment,stratify=data.sentiment,\\\n",
    "                                                   random_state=2019,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "***\n",
    "Now we need to actually convert our text into a numerical vector representation. We will use TF IDF from scikit-learn here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 92929) (10000, 92929)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(x_train)\n",
    "test_vectors = vectorizer.transform(x_test)\n",
    "\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Box Model\n",
    "***\n",
    "Now we will use the black box naive bayes model from scikit-learn to create a baseline model for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "***\n",
    "Now we use the basic accuracy as the metric for our classification, because our classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      5000\n",
      "           1       0.88      0.84      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = clf.predict(test_vectors)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Improved Sentiment Classifier using Deep Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture for the deep learning text classification model generally consists of the following connected components:\n",
    "\n",
    "Text(X) -> Embedding -> Deep Network (LSTM/GRU) -> Fully Connected (Dense) -> Output Layer (Softmax) -> Sentiment (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Text for Embedding Layer\n",
    "***\n",
    "The word embeddings can be learned while training a neural network on the classification problem. Before it can be presented to the network, the text data is encoded so that each word is represented by a unique integer. This can be performed using the **Tokenizer API** provided by **Keras**. \n",
    "\n",
    "We also add padding to make all of the vectors the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of any review: 2470\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.review.values)\n",
    "\n",
    "max_length = data.review.apply(lambda review: len(review.split())).max(axis=0)\n",
    "print(\"Maximum length of any review: {}\".format(max_length))\n",
    "\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_tokens = tokenizer.texts_to_sequences(x_train.values)\n",
    "test_tokens = tokenizer.texts_to_sequences(x_test.values)\n",
    "\n",
    "x_train_padded = pad_sequences(train_tokens, maxlen=max_length, padding=\"post\")\n",
    "x_test_padded = pad_sequences(test_tokens, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 17:18:27.048817 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0812 17:18:27.069983 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 17:18:27.072511 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0812 17:18:27.143563 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0812 17:18:27.153034 140149158770496 deprecation.py:506] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0812 17:18:27.408703 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 17:18:27.433365 140149158770496 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0812 17:18:27.438149 140149158770496 deprecation.py:323] From /home/joseph/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2470, 100)         12425300  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,438,101\n",
      "Trainable params: 12,438,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# dimension of semantic word vector space\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model using our training data and some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_padded, y_train.values, batch_size=400, epochs=5, \\\n",
    "          validation_data=(x_test_padded,y_test.values), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the model on some custom reviews that are clearly negative or positive to see how good it is at predicting the sentiment of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "test_sample_2 = \"Good movie!\"\n",
    "test_sample_3 = \"Maybe I like this movie.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
    "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
    "test_sample_6 = \"Bad movie!\"\n",
    "test_sample_7 = \"Not a good movie!\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6,\n",
    "               test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokenized = tokenizer.texts_to_sequences(test_samples)\n",
    "test_samples_padded = pad_sequences(test_samples_tokenized, maxlen=max_length)\n",
    "\n",
    "print(model.predict(x=test_samples_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec Embedding\n",
    "***\n",
    "Now we  will try to separately learn the word embeddings and then pass them to the embedding layer. This also allows us to use per-trained word embeddings. We will use `gensim` here. \n",
    "\n",
    "The **first step** is to prepare the corpus for the learning of the embeddings by **tokenization**, **removing punctuation**, **removing stop words**, etc. The word2vec algorithm processes documents sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuation_pattern = \"[\" + string.punctuation + \"]\"\n",
    "re.sub(pattern,\"\",my_string)\n",
    "\n",
    "cleaned_reviews = []\n",
    "reviews = data.review.values.tolist()\n",
    "\n",
    "for review in reviews:\n",
    "    # create tokens for this review\n",
    "    tokens = word_tokenize(review)\n",
    "    # convert all words to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # convert punctuation to empty space\n",
    "    tokens_less_punc = [re.sub(punctuation_pattern,\"\",token) for token in tokens]\n",
    "    # remove non alphabetic tokens\n",
    "    tokens = list(filter(str.isalpha, tokens_less_punc))\n",
    "    tokens = list(filter(lambda word:word not in stop_words, tokens))\n",
    "    cleaned_reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['original', 'movie', 'lovers', 'actually', 'love', 'show', 'stop', 'complaining', 'time', 'br', 'br', 'emperor', 'new', 'school', 'brings', 'old', 'jokes', 'movie', 'like', 'pulling', 'lever', 'yzma', 'lab', 'kuzco', 'pausing', 'episode', 'since', 'kids', 'show', 'classic', 'right', 'places', 'even', 'though', 'style', 'much', 'simple', 'animation', 'characters', 'keeps', 'personalities', 'well', 'surprised', 'actually', 'eartha', 'kitt', 'makes', 'excellent', 'voice', 'acting', 'yzma', 'jp', 'manoux', 'wonderful', 'job', 'kuzco', 'voice', 'instead', 'david', 'spade', 'played', 'kuzco', 'movie', 'great', 'plots', 'hilarious', 'moments', 'kuzco', 'amazing', 'looks', 'makes', 'show', 'worth', 'watching', 'stop', 'complaining', 'everything']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `gensim` to find our embeddings. The parameters needed for our Word2Vec model are:\n",
    "\n",
    "* `sentences` - a list of sentences\n",
    "* `size` - the number of dimensions for word vectors\n",
    "* `min_count` - minimum frequency for a word to be included\n",
    "* `window` - only terms that occur within a window neighborhood of a term in a sentence are associated\n",
    "* `workers` - number of threads used in training parallelization to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 16:19:34.207339 139901176153920 word2vec.py:1145] collecting all words and their counts\n",
      "I0812 16:19:34.208136 139901176153920 word2vec.py:1163] PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "I0812 16:19:34.405547 139901176153920 word2vec.py:1163] PROGRESS: at sentence #10000, processed 1214193 words, keeping 57422 word types\n",
      "I0812 16:19:34.596872 139901176153920 word2vec.py:1163] PROGRESS: at sentence #20000, processed 2418958 words, keeping 81729 word types\n",
      "I0812 16:19:34.814720 139901176153920 word2vec.py:1163] PROGRESS: at sentence #30000, processed 3641415 words, keeping 101927 word types\n",
      "I0812 16:19:35.047518 139901176153920 word2vec.py:1163] PROGRESS: at sentence #40000, processed 4890500 words, keeping 119311 word types\n",
      "I0812 16:19:35.301706 139901176153920 word2vec.py:1175] collected 134087 word types from a corpus of 6106047 raw words and 50000 sentences\n",
      "I0812 16:19:35.302377 139901176153920 word2vec.py:1208] Loading a fresh vocabulary\n",
      "I0812 16:19:35.721140 139901176153920 word2vec.py:1232] min_count=1 retains 134087 unique words (100% of original 134087, drops 0)\n",
      "I0812 16:19:35.721989 139901176153920 word2vec.py:1238] min_count=1 leaves 6106047 word corpus (100% of original 6106047, drops 0)\n",
      "I0812 16:19:36.128046 139901176153920 word2vec.py:1297] deleting the raw counts dictionary of 134087 items\n",
      "I0812 16:19:36.130763 139901176153920 word2vec.py:1300] sample=0.001 downsamples 22 most-common words\n",
      "I0812 16:19:36.131379 139901176153920 word2vec.py:1303] downsampling leaves estimated 5692248 word corpus (93.2% of prior 6106047)\n",
      "I0812 16:19:36.512017 139901176153920 base_any2vec.py:554] estimated required memory for 134087 words and 100 dimensions: 174313100 bytes\n",
      "I0812 16:19:36.512626 139901176153920 word2vec.py:1414] resetting layer weights\n",
      "I0812 16:19:37.892776 139901176153920 base_any2vec.py:624] training model with 8 workers on 134087 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "I0812 16:19:38.900483 139901176153920 base_any2vec.py:650] EPOCH 1 - PROGRESS: at 22.14% examples, 1254749 words/s, in_qsize 14, out_qsize 1\n",
      "I0812 16:19:39.918034 139901176153920 base_any2vec.py:650] EPOCH 1 - PROGRESS: at 46.26% examples, 1288694 words/s, in_qsize 16, out_qsize 0\n",
      "I0812 16:19:40.926519 139901176153920 base_any2vec.py:650] EPOCH 1 - PROGRESS: at 68.03% examples, 1278594 words/s, in_qsize 15, out_qsize 1\n",
      "I0812 16:19:41.929985 139901176153920 base_any2vec.py:650] EPOCH 1 - PROGRESS: at 90.24% examples, 1273871 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:42.306528 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 7 more threads\n",
      "I0812 16:19:42.310054 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 6 more threads\n",
      "I0812 16:19:42.316171 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 5 more threads\n",
      "I0812 16:19:42.317037 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 4 more threads\n",
      "I0812 16:19:42.325424 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 3 more threads\n",
      "I0812 16:19:42.334781 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 2 more threads\n",
      "I0812 16:19:42.336951 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 1 more threads\n",
      "I0812 16:19:42.338320 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 0 more threads\n",
      "I0812 16:19:42.338914 139901176153920 base_any2vec.py:664] EPOCH - 1 : training on 6106047 raw words (5692943 effective words) took 4.4s, 1281652 effective words/s\n",
      "I0812 16:19:43.361555 139901176153920 base_any2vec.py:650] EPOCH 2 - PROGRESS: at 23.48% examples, 1310521 words/s, in_qsize 14, out_qsize 1\n",
      "I0812 16:19:44.366256 139901176153920 base_any2vec.py:650] EPOCH 2 - PROGRESS: at 47.46% examples, 1319551 words/s, in_qsize 14, out_qsize 1\n",
      "I0812 16:19:45.372770 139901176153920 base_any2vec.py:650] EPOCH 2 - PROGRESS: at 69.73% examples, 1312464 words/s, in_qsize 16, out_qsize 2\n",
      "I0812 16:19:46.374665 139901176153920 base_any2vec.py:650] EPOCH 2 - PROGRESS: at 92.75% examples, 1311148 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:46.643024 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 7 more threads\n",
      "I0812 16:19:46.650196 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 6 more threads\n",
      "I0812 16:19:46.657163 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 5 more threads\n",
      "I0812 16:19:46.670060 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 4 more threads\n",
      "I0812 16:19:46.671306 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 3 more threads\n",
      "I0812 16:19:46.674216 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 2 more threads\n",
      "I0812 16:19:46.677785 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 1 more threads\n",
      "I0812 16:19:46.679527 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 0 more threads\n",
      "I0812 16:19:46.680175 139901176153920 base_any2vec.py:664] EPOCH - 2 : training on 6106047 raw words (5691990 effective words) took 4.3s, 1312811 effective words/s\n",
      "I0812 16:19:47.685192 139901176153920 base_any2vec.py:650] EPOCH 3 - PROGRESS: at 21.33% examples, 1210918 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:48.697444 139901176153920 base_any2vec.py:650] EPOCH 3 - PROGRESS: at 44.18% examples, 1233801 words/s, in_qsize 15, out_qsize 2\n",
      "I0812 16:19:49.700581 139901176153920 base_any2vec.py:650] EPOCH 3 - PROGRESS: at 67.06% examples, 1265445 words/s, in_qsize 16, out_qsize 0\n",
      "I0812 16:19:50.715460 139901176153920 base_any2vec.py:650] EPOCH 3 - PROGRESS: at 90.38% examples, 1276490 words/s, in_qsize 16, out_qsize 1\n",
      "I0812 16:19:51.091719 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 7 more threads\n",
      "I0812 16:19:51.093960 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 6 more threads\n",
      "I0812 16:19:51.109344 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 5 more threads\n",
      "I0812 16:19:51.110908 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 4 more threads\n",
      "I0812 16:19:51.114021 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 3 more threads\n",
      "I0812 16:19:51.119432 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 2 more threads\n",
      "I0812 16:19:51.121685 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 1 more threads\n",
      "I0812 16:19:51.131540 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 0 more threads\n",
      "I0812 16:19:51.132421 139901176153920 base_any2vec.py:664] EPOCH - 3 : training on 6106047 raw words (5691855 effective words) took 4.4s, 1279584 effective words/s\n",
      "I0812 16:19:52.139689 139901176153920 base_any2vec.py:650] EPOCH 4 - PROGRESS: at 23.01% examples, 1301608 words/s, in_qsize 16, out_qsize 1\n",
      "I0812 16:19:53.140116 139901176153920 base_any2vec.py:650] EPOCH 4 - PROGRESS: at 46.60% examples, 1309108 words/s, in_qsize 14, out_qsize 1\n",
      "I0812 16:19:54.143089 139901176153920 base_any2vec.py:650] EPOCH 4 - PROGRESS: at 68.36% examples, 1294589 words/s, in_qsize 16, out_qsize 0\n",
      "I0812 16:19:55.152357 139901176153920 base_any2vec.py:650] EPOCH 4 - PROGRESS: at 90.69% examples, 1286228 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:55.555710 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 7 more threads\n",
      "I0812 16:19:55.558538 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 6 more threads\n",
      "I0812 16:19:55.560260 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 5 more threads\n",
      "I0812 16:19:55.565571 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 4 more threads\n",
      "I0812 16:19:55.567389 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 3 more threads\n",
      "I0812 16:19:55.576122 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 16:19:55.582199 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 1 more threads\n",
      "I0812 16:19:55.585205 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 0 more threads\n",
      "I0812 16:19:55.585967 139901176153920 base_any2vec.py:664] EPOCH - 4 : training on 6106047 raw words (5692093 effective words) took 4.4s, 1279415 effective words/s\n",
      "I0812 16:19:56.603751 139901176153920 base_any2vec.py:650] EPOCH 5 - PROGRESS: at 21.01% examples, 1182179 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:57.616921 139901176153920 base_any2vec.py:650] EPOCH 5 - PROGRESS: at 42.99% examples, 1196022 words/s, in_qsize 16, out_qsize 1\n",
      "I0812 16:19:58.618730 139901176153920 base_any2vec.py:650] EPOCH 5 - PROGRESS: at 65.83% examples, 1237257 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:19:59.619908 139901176153920 base_any2vec.py:650] EPOCH 5 - PROGRESS: at 88.44% examples, 1250632 words/s, in_qsize 15, out_qsize 0\n",
      "I0812 16:20:00.100283 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 7 more threads\n",
      "I0812 16:20:00.104496 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 6 more threads\n",
      "I0812 16:20:00.115633 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 5 more threads\n",
      "I0812 16:20:00.120983 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 4 more threads\n",
      "I0812 16:20:00.125858 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 3 more threads\n",
      "I0812 16:20:00.129130 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 2 more threads\n",
      "I0812 16:20:00.131608 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 1 more threads\n",
      "I0812 16:20:00.144963 139901176153920 base_any2vec.py:178] worker thread finished; awaiting finish of 0 more threads\n",
      "I0812 16:20:00.146012 139901176153920 base_any2vec.py:664] EPOCH - 5 : training on 6106047 raw words (5691455 effective words) took 4.6s, 1250272 effective words/s\n",
      "I0812 16:20:00.147097 139901176153920 base_any2vec.py:682] training on a 30530235 raw words (28460336 effective words) took 22.3s, 1278903 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "gensim_model = gensim.models.Word2Vec( sentences=cleaned_reviews, \\\n",
    "                                      size=EMBEDDING_DIM, window=5, workers=8, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 134087\n"
     ]
    }
   ],
   "source": [
    "words = list(gensim_model.wv.vocab)\n",
    "print(\"Vocabulary size: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a peek at some of the word embeddings learned from the review dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 16:20:51.072980 139901176153920 keyedvectors.py:1042] precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.9216718673706055),\n",
       " ('awful', 0.8762881755828857),\n",
       " ('sucks', 0.7772316336631775),\n",
       " ('horrendous', 0.769995391368866),\n",
       " ('atrocious', 0.7531569600105286),\n",
       " ('pathetic', 0.7523128986358643),\n",
       " ('dreadful', 0.7506479024887085),\n",
       " ('horrid', 0.742202877998352),\n",
       " ('lousy', 0.7384706139564514),\n",
       " ('laughable', 0.7182028889656067)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar(\"horrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some of the math Word2Vec is famous for: `woman+king-man=?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.8810762166976929),\n",
       " ('romeo', 0.8673413991928101),\n",
       " ('prince', 0.8594143390655518),\n",
       " ('bride', 0.8522542119026184),\n",
       " ('queen', 0.850104808807373),\n",
       " ('onionpeeling', 0.8499622941017151),\n",
       " ('godmother', 0.8485676646232605),\n",
       " ('ammanda', 0.8458809852600098),\n",
       " ('furst', 0.8409291505813599),\n",
       " ('juliet', 0.8385201096534729)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.wv.most_similar_cosmul(positive=[\"woman\",\"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to use the word embeddings directly in the embedding layer in the sentiment classification model. We save the model to be used later:\n",
    "\n",
    "The first column of each row is the word in plaintext, and the rest of the columns are the components of the word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 16:23:18.838003 139901176153920 utils_any2vec.py:109] storing 134087x100 projection weights into data/imdb_embedding_word2vec.txt\n",
      "/home/joseph/miniconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(\"data/\",\"imdb_embedding_word2vec.txt\")\n",
    "gensim_model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the word embeddings from the stored file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(\"data/\",\"imdb_embedding_word2vec.txt\"), encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        # first column is the word\n",
    "        word = values[0]\n",
    "        # the rest of the columns in this row are the word vector components\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of any entry in this dictionary is a list of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-0.06289237', '-4.1104317', '-0.7192977', '0.3467713',\n",
       "       '0.15422522', '1.0290797', '0.29853466', '-0.4629418',\n",
       "       '-0.28074723', '-0.52995634', '-0.48216525', '-0.63887423',\n",
       "       '-0.61678463', '0.043714996', '1.1465778', '-0.0986907',\n",
       "       '-0.017181644', '-1.1504191', '-0.7751319', '-1.192805',\n",
       "       '0.1896599', '-0.78604263', '1.1917669', '0.14878462',\n",
       "       '-0.07003895', '0.15610346', '-0.6093981', '1.3712231',\n",
       "       '0.8185536', '-0.3403442', '-0.13370019', '-0.06265738',\n",
       "       '0.5446949', '-0.68191046', '1.0355029', '-0.2035864',\n",
       "       '-0.24064294', '-0.74360555', '0.56173146', '-1.7993212',\n",
       "       '-1.6746668', '1.3080883', '1.2252704', '-1.5737292', '-1.6902249',\n",
       "       '-0.3025173', '0.36642084', '0.4582488', '-0.45711353',\n",
       "       '-1.1066535', '0.45047405', '1.3150895', '-0.09814261',\n",
       "       '-0.5541568', '-1.140533', '-0.35467556', '0.5681579',\n",
       "       '-0.41108182', '-1.7969784', '0.26176956', '-0.21563247',\n",
       "       '-0.29490942', '0.012024802', '-0.41772678', '0.20120038',\n",
       "       '-0.37769943', '-1.2636132', '-0.5900609', '-0.45411107',\n",
       "       '-0.584477', '0.0112248575', '0.06903751', '-0.32660106',\n",
       "       '1.0146858', '0.6431395', '0.601542', '0.27921253', '-1.7728581',\n",
       "       '0.7512158', '-0.3060029', '-0.54076165', '0.44582742',\n",
       "       '-0.006137127', '0.19528788', '0.26935434', '1.163676',\n",
       "       '0.4746876', '0.08684326', '-0.29346955', '0.6904953',\n",
       "       '0.27940646', '-0.22974347', '0.64801145', '-2.4142194',\n",
       "       '0.41909817', '-1.2356291', '-1.8183051', '1.6334097',\n",
       "       '-2.1757958', '1.4678087'], dtype='<U12')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"br\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the word embedding into a tokenized vector. Recall that the review documents are integer encoded prior to passing them tot he embedding layer. THe integer maps the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map the correct vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134087 unique tokens\n",
      "Shape of reviews tensor: (50000, 2470)\n",
      "Shape of sentiment tensor: (50000,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_reviews)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment = data.sentiment.values\n",
    "\n",
    "print(\"Shape of reviews tensor: {}\".format(review_pad.shape))\n",
    "print(\"Shape of sentiment tensor: {}\".format(sentiment.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map the embeddings from the loaded word2vec model for each word to the `tokenizer.word_index` vocabulary and we create a matrix of word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) # same as __getitem__ but does checking\n",
    "    if embedding_vector is not None: # so that we can do this line\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    # else that row of embedding matrix will be all zeros cause we initialized it so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the embedded vector to be used directly in the embedding layer. In the below code, the only change from the previous model is that we are using `embedding_matrix` as input to the embedding layer and we are setting `trainable=False` because the embedding is already learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 16:39:54.945634 139901176153920 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 16:39:55.481924 139901176153920 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0812 16:39:55.804842 139901176153920 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0812 16:39:55.812100 139901176153920 deprecation.py:506] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0812 16:39:56.015739 139901176153920 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 16:39:56.037978 139901176153920 deprecation_wrapper.py:119] From /home/joseph/miniconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0812 16:39:56.044957 139901176153920 deprecation.py:323] From /home/joseph/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2470, 100)         13408800  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,421,601\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 13,408,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, \\\n",
    "                    EMBEDDING_DIM, \\\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\\\n",
    "                    input_length=max_length, \\\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you notice, we've cut down the `trainable_params` to be much lower than the total number of params. Therefore, since the model uses pre-trained word embeddings, we have few trainable parameters and it should train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(review_pad,sentiment,stratify=sentiment,\\\n",
    "                                                   random_state=2019,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=25, validation_data=(x_test, y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
