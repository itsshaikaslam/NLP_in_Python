{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some imports needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic MLP Toy Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int) - size of input vector\n",
    "            hidden_dim (int) - the size after first Linear Layer\n",
    "            output_dim (int) - size after second Linear Layer\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        compute forward pass\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor) - input data tensor. x_in.shape is (batch, input_dim)\n",
    "            apply_softmax (bool) - a flag for the softmax activation. \n",
    "                should be False if used with cross-entropy loss\n",
    "        Returns:\n",
    "            resulting tensor. tensor.shape is (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to instantiate an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 rows at once\n",
    "batch_size = 2\n",
    "# there are 3 original features\n",
    "input_dim = 3\n",
    "# 100 nodes in first hidden layer\n",
    "hidden_dim = 100\n",
    "# output 4 numbers\n",
    "output_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#initialize the Model\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the integrity to make sure we got the dimensions correctly by passing some random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.0797, 0.8508, 0.2240],\n",
      "        [0.5168, 0.9134, 0.5197]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 4])\n",
      "Values: \n",
      "tensor([[ 0.0599,  0.0319, -0.2909, -0.0025],\n",
      "        [ 0.1854, -0.0154, -0.4302,  0.0374]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))\n",
    "    \n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can convert each of the output vectors (each row) into a vector of probabilities by enabling the softmax activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 4])\n",
      "Values: \n",
      "tensor([[0.2766, 0.2689, 0.1947, 0.2598],\n",
      "        [0.3105, 0.2540, 0.1678, 0.2678]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_output_softmax = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to make sure that each of these 2 rows sums to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999999, 1.       ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output_softmax.detach().numpy().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Objective\n",
    "***\n",
    "Our objective in this project is to predict the nationality associated with a given last name. \n",
    "\n",
    "The original dataset contains 10000 surnames from 18 different nationalities. This dataset is imbalanced, as there is larger representation from certain nationalities than that of others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     34,
     48,
     64
    ]
   },
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame) - the dataset\n",
    "            vectorizer (SurnameVectorizer) - vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df['split']==\"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df['split']=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df['split']=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {\"train\": (self.train_df, self.train_size),\\\n",
    "                            \"val\": (self.val_df, self.validation_size),\\\n",
    "                            \"test\": (self.test_df, self.test_size)}\n",
    "        self.set_split(\"train\")\n",
    "        \n",
    "        counts_by_class = surname_df['nationality'].value_counts().to_dict()\n",
    "        \n",
    "        def sort_key(item):\n",
    "            return self._vectorizer['nationality_vocab'].lookup_token(item[0])\n",
    "        \n",
    "        sorted_counts = sorted(counts_by_class.items(), key=sort_key)\n",
    "        counts = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(counts, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"\n",
    "        loads a dataset and makes vectorizer\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str) - location of dataset\n",
    "        Returns:\n",
    "            SurnameDataset instance\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df['split']=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        load dataset and the vectorizier\n",
    "        use this when vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str) - location of dataset\n",
    "            vectorizer_filepath (str) - location of saved vectorizer\n",
    "        Returns:\n",
    "            SurnameDataset instance\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        static method for loading vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str) - location of serialized vectorizer\n",
    "        Returns:\n",
    "            SurnameVectorizer instance\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {\"x_surname\": surname_vector, \"y_nationality\": nationality_index}\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
