{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions to do the loading and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [
     3,
     11
    ]
   },
   "outputs": [],
   "source": [
    "punctuation_pattern = \"[\" + punctuation + \"]\"\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def load_doc( filename ):\n",
    "    \"\"\"\n",
    "    opens a file and returns the text inside\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fileHandle:\n",
    "        text = fileHandle.read()\n",
    "    return text\n",
    "\n",
    "def clean_doc( doc, minimum_token_length=2 ):\n",
    "    \"\"\"\n",
    "    reads in a string and returns a list of tokens\n",
    "    \"\"\"\n",
    "    tokens = doc.split()\n",
    "    tokens = [re.sub(punctuation_pattern,\"\",token).lower() for token in tokens if token not in stopwords\\\n",
    "             and len(token) >= minimum_token_length]\n",
    "    return tokens\n",
    "\n",
    "def process_docs( directory, vocab, is_train ):\n",
    "    \"\"\"\n",
    "    takes in a directory and a vocab, and reads the documents into a signle data structure\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if is_train and filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith(\"cv9\"):\n",
    "            continue\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        vocab.add_doc_to_vocab(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a vocabulary. The more words we have, the larger the representation of the documents. Therefore, it is important to constrain the words to only those believed to be predictive. It is difficult to know beforehand which words to use, and it is often necessary to test different hypotheses. \n",
    "\n",
    "We develop a vocabulary as a `Counter`, a dictionary mapping of words to their counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A wrapper around a collections.Counter\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab=None, minimum_token_length=2):\n",
    "        self._vocab_is_trained = False\n",
    "        if vocab is not None:\n",
    "            assert isinstance(vocab, Counter) \\\n",
    "                or isinstance(vocab, dict), \"If passing a vocab, must be of dict or counter type\"\n",
    "            self._vocab = Counter(vocab)\n",
    "            self._vocab_is_trained = True\n",
    "        else:\n",
    "            self._vocab = Counter()\n",
    "        self._minimum_token_length = minimum_token_length\n",
    "    \n",
    "    def fit_transform( self, filename ):\n",
    "        \"\"\"\n",
    "        this function reads in the file and adds tokens to the vocab\n",
    "        \"\"\"\n",
    "        doc = load_doc(filename)\n",
    "        tokens = self.clean_doc_under_vocab(doc)\n",
    "        self._vocab.update(tokens)\n",
    "    \n",
    "    def get_words(self):\n",
    "        return list(self._vocab.keys())\n",
    "    \n",
    "    def most_common(self,n=10):\n",
    "        return self._vocab.most_common(n)\n",
    "    \n",
    "    def write_to_file(self, outfilename):\n",
    "        with open(outfilename, \"w\") as outFileHandle:\n",
    "            data = \"\\n\".join(self.get_words())\n",
    "            outFileHandle.write(data)\n",
    "            \n",
    "    def clean_doc_under_vocab(self, doc):\n",
    "        \"\"\"\n",
    "        reads in a string and returns a list of tokens\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        tokens = [re.sub(punctuation_pattern,\"\",token).lower() for token in tokens if token not in stopwords\\\n",
    "                 and len(token) >= self._minimum_token_length and token in self.get_words()]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "process_docs(\"data/small_imdb_movie_reviews/txt_sentoken/neg\", vocab, True)\n",
    "process_docs(\"data/small_imdb_movie_reviews/txt_sentoken/pos\", vocab, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary: 45157\n",
      "film : 7983 counts\n",
      "one : 4946 counts\n",
      "movie : 4826 counts\n",
      "like : 3201 counts\n",
      "even : 2262 counts\n",
      "good : 2080 counts\n",
      "time : 2041 counts\n",
      "story : 1907 counts\n",
      "films : 1873 counts\n",
      "would : 1844 counts\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the vocabulary: {}\".format(len(vocab.get_words())))\n",
    "TOPN = 10\n",
    "most_common = vocab.most_common(TOPN)\n",
    "for word, count in most_common:\n",
    "    print(word, \": {} counts\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this vocabulary to a new file that we can later load and use to filter movie reviews before encoding them for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.write_to_file(\"data/small_imdb_movie_reviews/vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin training an embedding layer. You learn a word embedding by training a neural network on the classification problem. \n",
    "\n",
    "But before that, we need to load all of the training data movie reviews. We want each document to be a string for easy encoding as a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
