{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be Developing a Deep Convolutional Neural Network to Perform Sentiment Analysis on IMDB Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\",level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions to do the loading and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3,
     11
    ]
   },
   "outputs": [],
   "source": [
    "punctuation_pattern = \"[\" + punctuation + \"]\"\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def load_doc( filename ):\n",
    "    \"\"\"\n",
    "    opens a file and returns the text inside\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fileHandle:\n",
    "        text = fileHandle.read()\n",
    "    return text\n",
    "\n",
    "def process_docs( directory, vocab ):\n",
    "    \"\"\"\n",
    "    loop over files in a folder, skip reviews that are supposed to make up the test set,\n",
    "    load the file as document, clean the document text, then add cleaned document to documents.\n",
    "    return documents\n",
    "    Args:\n",
    "        directory (str): name of the directory containign documents (each in separate file)\n",
    "        vocab (Vocabulary): vocab to use to filter for words in the vocab\n",
    "    Returns:\n",
    "        documents (list): list of strings [documents]\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        doc = load_doc(full_path)\n",
    "        tokens = vocab.clean_doc(doc)\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a vocabulary. The more words we have, the larger the representation of the documents. Therefore, it is important to constrain the words to only those believed to be predictive. It is difficult to know beforehand which words to use, and it is often necessary to test different hypotheses. \n",
    "\n",
    "We develop a vocabulary as a `Counter`, a dictionary mapping of words to their counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4,
     15,
     26,
     29,
     35,
     40,
     50
    ]
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A wrapper around a collections.Counter\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab=None, minimum_token_length=2):\n",
    "        self._vocab_is_trained = False\n",
    "        if vocab is not None:\n",
    "            assert isinstance(vocab, Counter) \\\n",
    "                or isinstance(vocab, dict), \"If passing a vocab, must be of dict or counter type\"\n",
    "            self._vocab = Counter(vocab)\n",
    "            self._vocab_is_trained = True\n",
    "        else:\n",
    "            self._vocab = Counter()\n",
    "        self._minimum_token_length = minimum_token_length\n",
    "        \n",
    "    def fit(self, directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            # skip any reviews in test set\n",
    "            if is_train and filename.startswith(\"cv9\"):\n",
    "                continue\n",
    "            if not is_train and not filename.startswith(\"cv9\"):\n",
    "                continue\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        self.add_tokens(full_path)\n",
    "        self._vocab_is_trained = True\n",
    "    \n",
    "    def add_tokens(self, tokens):\n",
    "        self._vocab.update(tokens)\n",
    "        \n",
    "    def get_words(self):\n",
    "        return list(self._vocab.keys())\n",
    "    \n",
    "    def most_common(self,n=10):\n",
    "        return self._vocab.most_common(n)\n",
    "    \n",
    "    def write_to_file(self, outfilename):\n",
    "        with open(outfilename, \"w\") as outFileHandle:\n",
    "            data = \"\\n\".join(self.get_words())\n",
    "            outFileHandle.write(data)\n",
    "            \n",
    "    def clean_doc( self, doc, minimum_token_length=2 ):\n",
    "        \"\"\"\n",
    "        reads in a string and returns a list of tokens\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        tokens = [re.sub(punctuation_pattern,\"\",token).lower() for token in tokens if token not in stopwords\\\n",
    "                 and len(token) >= minimum_token_length]\n",
    "        tokens = \" \".join(tokens)\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45157"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data/small_imdb_movie_reviews/\"\n",
    "VOCAB_FILE = \"vocab.txt\"\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_DIR,VOCAB_FILE)):\n",
    "    with open(os.path.join(DATA_DIR,VOCAB_FILE)) as file:\n",
    "        tokens = file.readlines()\n",
    "        vocab = Vocabulary()\n",
    "        vocab.add_tokens(tokens)\n",
    "else:\n",
    "    vocab = Vocabulary()\n",
    "    vocab.fit(os.path.join(DATA_DIR,\"txt_sentoken/neg\"))\n",
    "    vocab.fit(os.path.join(DATA_DIR,\"txt_sentoken/pos\"))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save this vocabulary to a new file that we can later load and use to filter movie reviews before encoding them for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.write_to_file(os.path.join(DATA_DIR,VOCAB_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_documents = process_docs(\"data/small_imdb_movie_reviews/txt_sentoken/pos/\", vocab)\n",
    "negative_documents = process_docs(\"data/small_imdb_movie_reviews/txt_sentoken/neg/\", vocab)\n",
    "\n",
    "all_documents = negative_documents + positive_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a tokenizer and fit it on our training documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the documents as sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequences so they are all of the same length.\n",
    "\n",
    "These define the input training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(document) for document in all_documents])\n",
    "padded_sequences = pad_sequences(\n",
    "    encoded_docs, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0] * len(negative_documents) + [1] * len(positive_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, \\\n",
    "                                                    random_state=2019, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained stanford GloVe vectors from [here](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "Load the embedding from file and convert it into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     21
    ]
   },
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    takes in filename and returns a dictionary containing word to word vector\n",
    "    mappings. This assumes the text file containing the embeddings has the word\n",
    "    in the first column and then a space and then the vector\n",
    "    \n",
    "    Args:\n",
    "        filename (str): the name of the file containing the raw embeddings\n",
    "    Returns:\n",
    "        embedding (dict): mapping of words to their numpy vectors\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as fileHandle:\n",
    "        lines = fileHandle.readlines()\n",
    "    embedding = {}\n",
    "    for line in lines:\n",
    "        cols = line.split()\n",
    "        word = cols[0]\n",
    "        vector = cols[1:]\n",
    "        embedding[word] = np.array(vector, dtype=np.float32)\n",
    "    return embedding\n",
    "\n",
    "def convert_raw_embeddings_to_matrix(embedding, vocab):\n",
    "    \"\"\"\n",
    "    Creates a matrix of the words contained in the vocab\n",
    "\n",
    "    Args:\n",
    "        embedding (dict):\n",
    "        vocab (Vocabulary):\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)  # a \"None-safe\" __getitem__\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "        # else it stays zeroes\n",
    "        if (i % 15000 == 0):\n",
    "            logging.info(\"converted {} embeddings\".format(i))\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_embedding = load_embedding(\"data/glove_wikipedia_embeddings/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-22 13:52:19,162 : INFO : converted 1000 embeddings\n",
      "2019-08-22 13:52:19,165 : INFO : converted 2000 embeddings\n",
      "2019-08-22 13:52:19,169 : INFO : converted 3000 embeddings\n",
      "2019-08-22 13:52:19,172 : INFO : converted 4000 embeddings\n",
      "2019-08-22 13:52:19,176 : INFO : converted 5000 embeddings\n",
      "2019-08-22 13:52:19,179 : INFO : converted 6000 embeddings\n",
      "2019-08-22 13:52:19,183 : INFO : converted 7000 embeddings\n",
      "2019-08-22 13:52:19,187 : INFO : converted 8000 embeddings\n",
      "2019-08-22 13:52:19,189 : INFO : converted 9000 embeddings\n",
      "2019-08-22 13:52:19,192 : INFO : converted 10000 embeddings\n",
      "2019-08-22 13:52:19,196 : INFO : converted 11000 embeddings\n",
      "2019-08-22 13:52:19,199 : INFO : converted 12000 embeddings\n",
      "2019-08-22 13:52:19,203 : INFO : converted 13000 embeddings\n",
      "2019-08-22 13:52:19,206 : INFO : converted 14000 embeddings\n",
      "2019-08-22 13:52:19,210 : INFO : converted 15000 embeddings\n",
      "2019-08-22 13:52:19,213 : INFO : converted 16000 embeddings\n",
      "2019-08-22 13:52:19,217 : INFO : converted 17000 embeddings\n",
      "2019-08-22 13:52:19,220 : INFO : converted 18000 embeddings\n",
      "2019-08-22 13:52:19,224 : INFO : converted 19000 embeddings\n",
      "2019-08-22 13:52:19,226 : INFO : converted 20000 embeddings\n",
      "2019-08-22 13:52:19,229 : INFO : converted 21000 embeddings\n",
      "2019-08-22 13:52:19,231 : INFO : converted 22000 embeddings\n",
      "2019-08-22 13:52:19,234 : INFO : converted 23000 embeddings\n",
      "2019-08-22 13:52:19,237 : INFO : converted 24000 embeddings\n",
      "2019-08-22 13:52:19,240 : INFO : converted 25000 embeddings\n",
      "2019-08-22 13:52:19,242 : INFO : converted 26000 embeddings\n",
      "2019-08-22 13:52:19,244 : INFO : converted 27000 embeddings\n",
      "2019-08-22 13:52:19,247 : INFO : converted 28000 embeddings\n",
      "2019-08-22 13:52:19,250 : INFO : converted 29000 embeddings\n",
      "2019-08-22 13:52:19,252 : INFO : converted 30000 embeddings\n",
      "2019-08-22 13:52:19,254 : INFO : converted 31000 embeddings\n",
      "2019-08-22 13:52:19,256 : INFO : converted 32000 embeddings\n",
      "2019-08-22 13:52:19,258 : INFO : converted 33000 embeddings\n",
      "2019-08-22 13:52:19,260 : INFO : converted 34000 embeddings\n",
      "2019-08-22 13:52:19,262 : INFO : converted 35000 embeddings\n",
      "2019-08-22 13:52:19,265 : INFO : converted 36000 embeddings\n",
      "2019-08-22 13:52:19,268 : INFO : converted 37000 embeddings\n",
      "2019-08-22 13:52:19,270 : INFO : converted 38000 embeddings\n",
      "2019-08-22 13:52:19,273 : INFO : converted 39000 embeddings\n",
      "2019-08-22 13:52:19,276 : INFO : converted 40000 embeddings\n",
      "2019-08-22 13:52:19,278 : INFO : converted 41000 embeddings\n",
      "2019-08-22 13:52:19,280 : INFO : converted 42000 embeddings\n",
      "2019-08-22 13:52:19,282 : INFO : converted 43000 embeddings\n",
      "2019-08-22 13:52:19,284 : INFO : converted 44000 embeddings\n",
      "2019-08-22 13:52:19,286 : INFO : converted 45000 embeddings\n",
      "2019-08-22 13:52:19,288 : INFO : converted 46000 embeddings\n",
      "2019-08-22 13:52:19,290 : INFO : converted 47000 embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_vectors = convert_raw_embeddings_to_matrix(raw_embedding, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], \\\n",
    "                           input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10498, 100)        4748800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 10494, 128)        64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5247, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 671616)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 671617    \n",
      "=================================================================\n",
      "Total params: 5,484,545\n",
      "Trainable params: 735,745\n",
      "Non-trainable params: 4,748,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
