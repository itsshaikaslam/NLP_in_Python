{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "One of the most common applications of NLP is sentiment analysis. From opinion polls to creating entire marketing strategies, this domain has completely reshaped the way businesses work.\n",
    "\n",
    "Thousands of text documents can be processed for sentiment (and other features such as named entities, topics, themes, etc.) in seconds.\n",
    "\n",
    "In this article, we solve the Twitter Sentiment Analysis Practice Problem. We follow a sequence of steps needed to solve a general sentiment analysis problem. We start with preprocessing and cleaning the raw text of the tweets.\n",
    "\n",
    "Then, we'll explore the cleaned text and try to get some intuition about the context of the tweets. \n",
    "\n",
    "After that, we will extract numerical features from the data and use them to train models and identify the sentiments of the tweets.\n",
    "# Understand the Problem Statement\n",
    "***\n",
    "The object of this task is to **identify hate speech in tweets**. For the sake of simplicity, we say a tweet **contains hate speech if it has a racist or sexist sentiment associated with it.** So in other words, the task is to **classify tweets as racist or sexists or neither**. \n",
    "# Tweet Preprocessing and Cleaning\n",
    "***\n",
    "Cleaning the data is necessary in order to remove noise in the data and make it more consistent. We clean noise that is less relevant to find the sentiment of tweets such as punctuation, special characters, numbers and terms that don't carry explanatory weight in general. \n",
    "# The Data\n",
    "***\n",
    "You can download the data from [here](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)\n",
    "\n",
    "The collection of tweets is split in the ratio of 65:35 in training to testing. Out of the testing set, 30% is public and the rest is private. \n",
    "\n",
    "The training set is contained in **train.csv** which contains 31,962 labelled tweets. Each line contains the tweet id, its label and the tweet. \n",
    "\n",
    "And the testing set is contained in **test_tweets.csv** which contains tweet ids and the tweet text.\n",
    "\n",
    "Here are the imports we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/hate_speech_tweet_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a helper function for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data( data_dir ):\n",
    "    train_file_name = \"tweet_train_set.csv\"\n",
    "    test_file_name = \"tweet_test_set.csv\"\n",
    "    train_set = pd.read_csv(os.path.join(data_dir,train_file_name))\n",
    "    test_set = pd.read_csv(os.path.join(data_dir,test_file_name))\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what one tweet looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = load_data( DATA_DIR )\n",
    "x_train.iloc[0]['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have the tweet id, the label and tweet text. The label here is 0 for innocent, or 1 for hate speech.\n",
    "\n",
    "Here's some data cleaning steps we must perform:\n",
    "\n",
    "* the twitter handles are masked as @user for anonymity, so they don't provide any information.\n",
    "* we could try getting rid of punctuations, numbers and special characters\n",
    "* we can remove small words with little meaning, such as \"pdx\", \"his\" and \"all\"\n",
    "* then we tokenize the tweets into tokens\n",
    "* then we perform lemmatization. so ('loves','loving','lovable') -> 'love' to reduce the total number of unique words in the dataset without losing so much information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Anonymized Twitter Handles (@user)\n",
    "***\n",
    "We define a helper function that removes an unwanted pattern of text from an input string and returns the original string without the unwanted pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_pattern( input_txt, pattern ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_txt (str) - the input text to work on\n",
    "        pattern (str) - the regex pattern to remove from the text\n",
    "    Returns:\n",
    "        input_txt (str) - the original text without the unwanted pattern\n",
    "    \"\"\"\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for match in r:\n",
    "        input_txt = re.sub(match, \"\", input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new column containing the \"tidied\" tweet. \n",
    "\n",
    "Our pattern matches any word that starts with an '@', followed by any number of letters '[\\w]*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_twitter_handle_pattern = \"@[\\w]*\"\n",
    "\n",
    "x_train[\"tidy_tweet\"] = x_train[\"tweet\"].apply(\n",
    "    remove_pattern, args=(anonymized_twitter_handle_pattern,))\n",
    "x_test[\"tidy_tweet\"] = x_test[\"tweet\"].apply(\n",
    "    remove_pattern, args=(anonymized_twitter_handle_pattern,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at our \"tidied\" tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet\n",
      "--------------\n",
      " @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Tidied tweet\n",
      "--------------\n",
      "  when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tweet\")\n",
    "print(\"--------------\")\n",
    "print(x_train['tweet'].iloc[0])\n",
    "print(\"Tidied tweet\")\n",
    "print(\"--------------\")\n",
    "print(x_train['tidy_tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations, Numbers and Special Characters\n",
    "***\n",
    "We can access the \"str\" attribute of a pandas Series of dtype object to use the built in string methods for pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anything_not_characters_nor_pound = \"[^a-zA-Z#]\"\n",
    "x_train[\"tidy_tweet\"] = x_train[\"tidy_tweet\"].str.replace(anything_not_characters_nor_pound, \" \")\n",
    "x_test[\"tidy_tweet\"] = x_test[\"tidy_tweet\"].str.replace(anything_not_characters_nor_pound, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Short Words\n",
    "***\n",
    "We can remove all words having length 3 or less. We could also swap this step out for removing stop words using an established package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_words_less_or_equal_three_characters( text ):\n",
    "    return \" \".join([word for word in text.split() if len(word) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"tidy_tweet\"] = x_train[\"tidy_tweet\"].apply(remove_words_less_or_equal_three_characters)\n",
    "x_test[\"tidy_tweet\"] = x_test[\"tidy_tweet\"].apply(remove_words_less_or_equal_three_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the tidied tweets versus the original tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet\n",
      "--------------\n",
      " @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Tidied tweet\n",
      "--------------\n",
      "when father dysfunctional selfish drags kids into dysfunction #run\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tweet\")\n",
    "print(\"--------------\")\n",
    "print(x_train['tweet'].iloc[0])\n",
    "print(\"Tidied tweet\")\n",
    "print(\"--------------\")\n",
    "print(x_train['tidy_tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "***\n",
    "Now we tokenize the cleaned tweets in our dataset. We split the string of a text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'father', 'dysfunctional', 'selfish', 'drags', 'kids', 'into', 'dysfunction', '#run']\n"
     ]
    }
   ],
   "source": [
    "x_train[\"tokenized_tidy_tweet\"] = x_train[\"tidy_tweet\"].apply(lambda tweet: tweet.split())\n",
    "x_test[\"tokenized_tidy_tweet\"] = x_test[\"tidy_tweet\"].apply(lambda tweet: tweet.split())\n",
    "\n",
    "print(x_train[\"tokenized_tidy_tweet\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "***\n",
    "Stemming is a rule-based process of stripping the suffixes (\"ing\",\"ly\",\"es\",\"s\", etc.) from a word. This way, \"play\",\"player\",\"played\",\"plays\", and \"playing\" all reduce to \"play\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'father', 'dysfunct', 'selfish', 'drag', 'kid', 'into', 'dysfunct', '#run']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "x_train[\"tokenized_tidy_tweet\"] = x_train[\"tokenized_tidy_tweet\"].\\\n",
    "    apply(lambda tweet: [stemmer.stem(token) for token in tweet])\n",
    "x_test[\"tokenized_tidy_tweet\"] = x_test[\"tokenized_tidy_tweet\"].\\\n",
    "    apply(lambda tweet: [stemmer.stem(token) for token in tweet])\n",
    "\n",
    "print(x_train[\"tokenized_tidy_tweet\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it All Together\n",
    "***\n",
    "Now that we've done a bunch of preprocessing, we can put all of our lists of tokens back into contiguous strings. This way we can perform our vectorization of the strings as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"tidy_tweet\"] = x_train[\"tidy_tweet\"].apply(lambda token_list: \" \".join(token_list))\n",
    "x_test[\"tidy_tweet\"] = x_test[\"tidy_tweet\"].apply(lambda token_list: \" \".join(token_list))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
